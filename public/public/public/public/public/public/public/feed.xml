<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://blog.rifhanakram.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.rifhanakram.com/" rel="alternate" type="text/html" /><updated>2025-04-14T16:12:04+05:30</updated><id>https://blog.rifhanakram.com/feed.xml</id><title type="html">Scribbles by Rifhan</title><subtitle>Scribbles by Rifhan is a personal blog</subtitle><author><name>Rifhan Akram</name></author><entry><title type="html">Lessons From ER To The Software World</title><link href="https://blog.rifhanakram.com/posts/lessons-from-er-to-software-world/" rel="alternate" type="text/html" title="Lessons From ER To The Software World" /><published>2025-04-14T00:00:00+05:30</published><updated>2025-04-14T00:00:00+05:30</updated><id>https://blog.rifhanakram.com/posts/lessons-from-er-to-software-world</id><content type="html" xml:base="https://blog.rifhanakram.com/posts/lessons-from-er-to-software-world/">&lt;p&gt;What can software engineers learn from emergency rooms? In this article, I explore how the principles of ER operations—from triaging and handling surges to conducting blameless postmortems—can be applied to software incident response. Drawing parallels between medical emergencies and software incidents, I discuss the importance of proper logging, structured incident response playbooks, and continuous improvement through postmortems. The article provides practical insights for teams of any size to build more reliable software systems using lessons from emergency medicine.&lt;/p&gt;

&lt;p&gt;Read the full article on &lt;a href=&quot;https://engineering.corzent.com/when-things-go-wrong-lessons-from-er-emergency-room-to-the-software-world-9a28881c3b02&quot;&gt;Corzent’s Engineering Blog&lt;/a&gt;.&lt;/p&gt;</content><author><name>Rifhan Akram</name></author><category term="tech" /><summary type="html">What can software engineers learn from emergency rooms? In this article, I explore how the principles of ER operations—from triaging and handling surges to conducting blameless postmortems—can be applied to software incident response. Drawing parallels between medical emergencies and software incidents, I discuss the importance of proper logging, structured incident response playbooks, and continuous improvement through postmortems. The article provides practical insights for teams of any size to build more reliable software systems using lessons from emergency medicine.</summary></entry><entry><title type="html">UUIDs vs ULIDs For Better Write Performance At Scale</title><link href="https://blog.rifhanakram.com/posts/ulids-vs-uuids-for-better-write-perf/" rel="alternate" type="text/html" title="UUIDs vs ULIDs For Better Write Performance At Scale" /><published>2023-01-14T00:00:00+05:30</published><updated>2023-01-14T00:00:00+05:30</updated><id>https://blog.rifhanakram.com/posts/ulids-vs-uuids-for-better-write-perf</id><content type="html" xml:base="https://blog.rifhanakram.com/posts/ulids-vs-uuids-for-better-write-perf/">&lt;h1 id=&quot;uuids-vs-ulids-for-better-write-performance-at-scale&quot;&gt;UUIDs vs ULIDs For Better Write Performance At Scale&lt;/h1&gt;

&lt;p&gt;In this article, we will be discussing the usage of keys as unique identifiers in a distributed relational database setup and their impact on write performance.&lt;/p&gt;

&lt;p&gt;The use of auto-incremented, non-repeating, sorted integers has been a common practice for defining primary keys in relational databases. This type of key has been successful because it satisfies the two most crucial indexing requirements.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They are unique, so they can be uniquely identified.&lt;/li&gt;
  &lt;li&gt;They are ordered and sortable, so they can be efficiently indexed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When the database is a monolithic entity, this kind of key is ideal. However, as soon as we decide to scale and move to a distributed setup this is no longer unique, this is because now multiple entities start generating auto-incremented numbers.&lt;/p&gt;

&lt;h2 id=&quot;impact-on-clustered-indexes-during-writes&quot;&gt;Impact on Clustered Indexes During Writes&lt;/h2&gt;

&lt;p&gt;Before we move on to discuss alternatives to auto-incremented integers as unique identifiers, let’s try to understand how indexes are impacted when relational databases write new data.&lt;/p&gt;

&lt;p&gt;Clustered indexes are typically represented in a b-tree data structure. They have several benefits that make them well-suited for usage in clustered indexes.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Efficient at searching for data: B-trees are designed to be efficient at searching for data, even in large datasets. They use a hierarchical structure that allows for fast traversal of the data, reducing the number of disk I/O operations required to find a specific value.&lt;/li&gt;
  &lt;li&gt;B-trees are balanced: B-trees are self-balancing data structures, which means they maintain a balanced tree structure even when data is inserted or deleted. This helps to ensure that the height of the tree remains relatively constant, which improves performance and reduces the risk of data becoming inaccessible.&lt;/li&gt;
  &lt;li&gt;B-trees support range queries: B-trees can support range queries which means they can be used to efficiently find all the values that fall within a specified range. This is useful in scenarios where you need to find all the records that fall within a certain range of values for a particular column.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In most cases in relational databases, it is allowed to have only 1 clustered index per table, this is because the physical organization of the data is controlled by the clustered index(by the column(s) the index is created, typically the primary key column).&lt;/p&gt;

&lt;p&gt;In MSSQL when writing data to a table the DBMS includes logical and physical writes. A logical write is when data is modified in a page in the buffer cache and marked as dirty. A physical write is when a dirty page is written from the buffer cache to the disk. Since logical writes are not immediately written to disk there can be more than one logical write to a page in the buffer cache as long as the record is intended to be written on that page(determined by the clustered index).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/sql-writes.png&quot; alt=&quot;sql-writes.png&quot; /&gt;
&lt;em&gt;Reference - &lt;a href=&quot;https://learn.microsoft.com/en-us/sql/relational-databases/writing-pages?view=sql-server-ver16&quot;&gt;Writing Pages - SQL Server | Microsoft Learn&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The DBMS must first determine the correct page to store the new row of data based on the clustered index. To do this, it typically performs a binary search on the index to identify the correct page, once the page is identified it needs to read the page from the disk into the buffer cache, this is only required if the page is not present in the buffer cache.&lt;/p&gt;

&lt;p&gt;If the clustered index key adheres to the indexing requirements discussed above then subsequent writes will not require reading pages from the physical disk as in most cases they will be available in the buffer cache due to a previous write in near time reading it into the buffer cache.&lt;/p&gt;

&lt;p&gt;As soon as you start using a column that is not ordered(during generation) for a clustered index; It would mean that the data stored in the table requires additional work to be organized in a logical order, which would result in poor write performance.&lt;/p&gt;

&lt;h2 id=&quot;alternative-unique-identifiers-in-a-distributed-environment&quot;&gt;Alternative Unique Identifiers In a Distributed Environment&lt;/h2&gt;

&lt;h3 id=&quot;universally-unique-identifier-uuid&quot;&gt;Universally Unique Identifier (UUID)&lt;/h3&gt;

&lt;p&gt;UUIDs are 128-bit long strings that can guarantee
uniqueness across space and time. They are widely used to uniquely identify resources. There are several versions of UUIDs, each with a slightly different format. The most generally used UUID version is 4.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/uuid.png&quot; alt=&quot;uuid.png&quot; /&gt;
&lt;em&gt;128 bit UUIDV4 string&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A more detailed explanation of UUID can be found in &lt;a href=&quot;https://www.ietf.org/rfc/rfc4122.txt&quot;&gt;RFC 4122&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;UUID in a distributed setup allows uniqueness but they are not so good for write performance and here is why;&lt;/p&gt;

&lt;p&gt;UUIDs are randomized strings(in most versions) that have no particular order in a generation. When you use them for a clustered index column they need to be ordered and stored. Since they are not ordered(naturally during generation) it requires more I/O to store them on the correct page, this could result in bad write performance in large tables and it could lead to issues like &lt;a href=&quot;https://learn.microsoft.com/en-us/sql/relational-databases/indexes/reorganize-and-rebuild-indexes?view=sql-server-ver16&quot;&gt;index fragmentation&lt;/a&gt;  as the data pages may not be in contiguous order.&lt;/p&gt;

&lt;h3 id=&quot;universally-unique-lexicographically-sortable-identifier-ulid&quot;&gt;Universally Unique Lexicographically Sortable Identifier (ULID)&lt;/h3&gt;

&lt;p&gt;As the title suggests ULIDs are universally unique yet lexicographically sortable identifiers, this is mainly why it is better in write performance at scale compared to UUIDs. ULIDs are a relatively new form of identifiers and still lack widespread native support.&lt;/p&gt;

&lt;p&gt;ULID is generated in 2 components,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Timestamp - 48-bit, integer with the UNIX-time in milliseconds&lt;/li&gt;
  &lt;li&gt;Randomness - 80-bit random string&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In total, they are a 128-bit long unique yet lexicographically sortable string. ULID is encoded using a combination of binary and base32 characters which results in a more compact 26-character format compared to the 36 characters generated by UUID v4.&lt;/p&gt;

&lt;p&gt;More details about ULID can be found in its &lt;a href=&quot;[ulid/spec: The canonical spec for ulid (github.com)](https://github.com/ulid/spec)&quot;&gt;specification document&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In general, we can conclude that ULIDs tend to have the upper hand in write performance( in most cases ) compared to UUIDs, specifically due to the lexicographically sortable nature of ULID.&lt;/p&gt;

&lt;p&gt;Thank you for reading this article. I hope to see you at the next one.&lt;/p&gt;</content><author><name>Rifhan Akram</name></author><category term="tech" /><summary type="html">UUIDs vs ULIDs For Better Write Performance At Scale</summary></entry><entry><title type="html">Automated CI/CD for Monorepo’s</title><link href="https://blog.rifhanakram.com/posts/jenkins-pipeline-monorepo/" rel="alternate" type="text/html" title="Automated CI/CD for Monorepo’s" /><published>2020-10-27T00:00:00+05:30</published><updated>2020-10-27T00:00:00+05:30</updated><id>https://blog.rifhanakram.com/posts/jenkins-pipeline-monorepo</id><content type="html" xml:base="https://blog.rifhanakram.com/posts/jenkins-pipeline-monorepo/">&lt;p&gt;Monorepo is an approach in managing source code under a product, team or company within a single repo as oppose to multi-repo where a single product/application is contained within its own git repository.&lt;/p&gt;

&lt;p&gt;The two approaches has its own pro’s and con’s. In this article i will not be discussing on what is the best approach as it entirely matters on the context we work, rather i will walk the through the challenges of Automated CI/CD with &lt;strong&gt;mono-repo&lt;/strong&gt; and how we can over come them with structure and some magic with &lt;a href=&quot;https://www.jenkins.io/doc/book/pipeline/&quot;&gt;Jenkins Pipeline&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;p&gt;Due to the nature of multiple applications residing in a single repo it is not trivial to setup a CI/CD job that detects changes and executes only the &lt;strong&gt;impacted applications related CI/CD job&lt;/strong&gt;. Some of the steps that contain in a typical pipeline are,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Building the code&lt;/li&gt;
  &lt;li&gt;Running automated tests&lt;/li&gt;
  &lt;li&gt;Creating and versioning a deployable artifact&lt;/li&gt;
  &lt;li&gt;Deploying that artifact&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lets look at how we can tackle this problem with Jenkins.&lt;/p&gt;

&lt;h2 id=&quot;some-jenkins-magic&quot;&gt;Some Jenkins Magic&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;sample structure in a mono-repo setup with jenkins pipeline&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── src                                 
    ├── react-app
        ├── Jenkinsfile     
    ├── node-app
        ├── Jenkinsfile                                                               
└── Jenkinsfile                 # main jenkinsfile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we consider a structure like above, with jenkins we could setup the main Jenkinsfile in the root of the repository as a &lt;a href=&quot;https://www.jenkins.io/doc/tutorials/build-a-multibranch-pipeline-project/&quot;&gt;multi-branch jenkins job&lt;/a&gt;. This job can be connected with your remote repository such that it triggers via a webhook when a PR is merged to your mainline branch. You can get creative with the triggers depending on the workflow your team follows.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;A stage inside the main Jenkinsfile is shown below&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;stage(&apos;Execute Jobs&apos;) {
    failFast false
    parallel {
        stage(&apos;react-app ci&apos;) {
            when {
                changeset &quot;src/react-app/**&quot;
            }
            steps {
                build job: &quot;react-app-job&quot;, parameters: [string(name: &apos;branch_name&apos;, value: env.BRANCH_NAME)]
            }
        }
        stage(&apos;node-app ci&apos;) {
            when {
                changeset &quot;src/node-app/**&quot;
            }
            steps {
                build job: &quot;node-app-job&quot;, parameters: [string(name: &apos;branch_name&apos;, value: env.BRANCH_NAME)]
            }
        }
    }
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In the above example the &lt;strong&gt;changeset&lt;/strong&gt; condition walks through the git changelog for that particular change and checks if there is any change within the given path. If a change is detected then the &lt;strong&gt;build job&lt;/strong&gt; step executes the related jenkins job.&lt;/p&gt;

&lt;p&gt;Note - &lt;em&gt;&lt;strong&gt;failFast&lt;/strong&gt; is set to false to avoid failure of the entire pipeline if one job fails.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once we have this setup we can go about and setup separate pipeline jobs for each application in the repository.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Below is an example on how the main multi-branch job will look in Jenkins&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/jenkins-multi-main.png&quot; alt=&quot;jenkins-multi-main.png&quot; /&gt;
In the above image the multi-branch job has executed when a change has been pushed to the master branch. When we navigate into the master branch execution we can see the pipeline as shown below,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/jenkins-multi-stage-view.png&quot; alt=&quot;jenkins-multi-stage-view.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the stage view we can see that build #9 contains changes only for 1 application and build #10 contains changes for both.&lt;/p&gt;

&lt;p&gt;This is an awesome and quick setup with Jenkins for a monorepo. I’m a big fan of Jenkins due to its flexibility and extensibility specially with declarative &lt;a href=&quot;https://www.jenkins.io/doc/book/pipeline/&quot;&gt;Jenkins Pipeline&lt;/a&gt;.&lt;/p&gt;</content><author><name>Rifhan Akram</name></author><category term="tech" /><summary type="html">Monorepo is an approach in managing source code under a product, team or company within a single repo as oppose to multi-repo where a single product/application is contained within its own git repository.</summary></entry><entry><title type="html">Tech Talks</title><link href="https://blog.rifhanakram.com/posts/tech-talks/" rel="alternate" type="text/html" title="Tech Talks" /><published>2020-10-27T00:00:00+05:30</published><updated>2020-10-27T00:00:00+05:30</updated><id>https://blog.rifhanakram.com/posts/tech-talks</id><content type="html" xml:base="https://blog.rifhanakram.com/posts/tech-talks/">&lt;h2 id=&quot;grpc-microservice-communication-with-aws-app-mesh&quot;&gt;gRPC Microservice Communication With AWS App Mesh&lt;/h2&gt;

&lt;p&gt;This talk was done at the 2020 July AWS User Group Colombo meetup. In this talk i walk through the challenges of microservice communication with gRPC and how we can use AWS App mesh to manage this communication and overcome the challenges mentioned.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qXeBwMVCnSw&quot;&gt;Video&lt;/a&gt; (July 2020)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://speakerdeck.com/rifhanakram/grpc-microservice-communication-with-aws-appmesh&quot;&gt;Slides&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/rifhanakram/gRPC-appmesh-demo&quot;&gt;Code Samples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;net-diaries-ep-1-the-evolution-of-net--best-practices-of-net-core&quot;&gt;.Net Diaries Ep 1: The Evolution of .Net &amp;amp; Best Practices of .Net Core&lt;/h2&gt;

&lt;p&gt;I participated in a panel discussion hosted by Gapstars for Episode 1 of .Net Diaries, where we engaged in an in-depth conversation about the evolution of the .Net framework and best practices in .Net Core. The panel explored the framework’s transformation over the years and how .Net Core has revolutionized the development landscape. We collectively shared insights about key benefits like cross-platform capabilities, improved performance, and modern development practices, while also discussing potential challenges teams might face during migration and adoption.&lt;/p&gt;

&lt;h3 id=&quot;references-1&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=VyL0WOS86Nc&quot;&gt;Video&lt;/a&gt; (March 2023)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks to Gapstars for organizing this panel discussion and bringing together .Net practitioners to share their experiences and insights with the community.&lt;/p&gt;

&lt;h2 id=&quot;net-diaries-ep-2-going-cloud-native-with-net&quot;&gt;.Net Diaries Ep 2: Going Cloud Native with .Net&lt;/h2&gt;

&lt;p&gt;I participated in the second episode of .Net Diaries hosted by Gapstars, where we explored the world of cloud-native .Net applications. The panel discussion focused on key aspects of modern .Net development in the cloud, including scalability, performance optimization, and security best practices. We shared practical insights and real-world experiences about building and deploying .Net applications in cloud environments.&lt;/p&gt;

&lt;h3 id=&quot;references-2&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=TSQo109afMk&quot;&gt;Video&lt;/a&gt; (August 2023)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks to Gapstars for continuing this insightful series and providing a platform for the .Net community to share knowledge and experiences.&lt;/p&gt;</content><author><name>Rifhan Akram</name></author><category term="tech" /><summary type="html">gRPC Microservice Communication With AWS App Mesh</summary></entry></feed>