<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://blog.rifhanakram.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.rifhanakram.com/" rel="alternate" type="text/html" /><updated>2025-09-16T13:43:50+05:30</updated><id>https://blog.rifhanakram.com/feed.xml</id><title type="html">Scribbles by Rifhan</title><subtitle>Scribbles by Rifhan is a personal blog</subtitle><author><name>Rifhan Akram</name></author><entry><title type="html">My AWS IAM Playbook: 7 Lessons from the Trenches</title><link href="https://blog.rifhanakram.com/posts/IAM-what-IAM-secure-your-aws-kingdom/" rel="alternate" type="text/html" title="My AWS IAM Playbook: 7 Lessons from the Trenches" /><published>2025-09-16T00:00:00+05:30</published><updated>2025-09-16T00:00:00+05:30</updated><id>https://blog.rifhanakram.com/posts/IAM-what-IAM-secure-your-aws-kingdom</id><content type="html" xml:base="https://blog.rifhanakram.com/posts/IAM-what-IAM-secure-your-aws-kingdom/">&lt;h1 id=&quot;my-aws-iam-playbook-7-lessons-from-the-trenches&quot;&gt;My AWS IAM Playbook: 7 Lessons from the Trenches&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/my-iam-playbook.png&quot; alt=&quot;Panicked engineers after production is compromised&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After years of working in software companies and startups building and scaling on AWS, I’ve seen a clear pattern. The teams that move fastest and sleep best at night are the ones who took Identity and Access Management (IAM) seriously from day one. It’s the bedrock of a secure and scalable cloud environment.&lt;/p&gt;

&lt;p&gt;Based on my experience, this isn’t just a list of best practices. This is my non-negotiable playbook for getting IAM right from the day one.&lt;/p&gt;

&lt;h3 id=&quot;1-the-first-thing-i-check-is-mfa-on-everything&quot;&gt;1. The First Thing I Check: Is MFA on Everything?&lt;/h3&gt;

&lt;p&gt;When I start working with a client, the very first thing I look at is the MFA status. It tells alot about security posture and thinking. Seeing a root account without Multi-Factor Authentication is a red flag.&lt;/p&gt;

&lt;p&gt;A password is just a single, fragile key. MFA is the deadbolt. If a password gets compromised (and it will), MFA is the only thing standing between an attacker and your entire AWS kingdom. My rule is simple: MFA isn’t just for the root user. It’s for &lt;strong&gt;every single human user&lt;/strong&gt; in the account. No exceptions.&lt;/p&gt;

&lt;h3 id=&quot;2-dont-learn-this-lesson-the-hard-way-rotate-your-access-keys&quot;&gt;2. Don’t Learn This Lesson the Hard Way: Rotate Your Access Keys&lt;/h3&gt;

&lt;p&gt;I once worked with a company that was dealing with a account being compromised and unidentified compute resources running. It turned out to be an old access key from a developer who had left the company six months prior. The key, which had been accidentally committed to a private GitHub repo that was later made public, was still active.&lt;/p&gt;

&lt;p&gt;Long-lived, static credentials are a ticking time bomb. My advice is firm: enforce a &lt;strong&gt;90-day rotation policy&lt;/strong&gt; for all programmatic access keys. If a key is leaked, this rule ensures it becomes useless in a matter of weeks, not years. Use tools like IAM Access Analyzer to find and delete old, unused keys—it’s one of the easiest security wins you can get.&lt;/p&gt;

&lt;h3 id=&quot;3-the-most-dangerous-habit-i-see-using-the-root-user&quot;&gt;3. The Most Dangerous Habit I See: Using the Root User&lt;/h3&gt;

&lt;p&gt;I still see founders and key stakeholders using the root account for daily tasks. This is like using the master key for a skyscraper to open your office door every day. It’s unnecessarily risky.&lt;/p&gt;

&lt;p&gt;The root user is a “break glass in case of emergency” account. My process with every AWS org is the same:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Put MFA on the root account immediately.&lt;/li&gt;
  &lt;li&gt;Create a dedicated IAM user with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AdministratorAccess&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Use &lt;em&gt;that&lt;/em&gt; user for daily admin work.&lt;/li&gt;
  &lt;li&gt;Lock the root credentials away in a secure vault.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You should only touch the root user for a handful of specific billing or account-level tasks. Period.&lt;/p&gt;

&lt;h3 id=&quot;4-stop-the-privilege-creep-before-it-starts-use-roles-not-individuals&quot;&gt;4. Stop the “Privilege Creep” Before It Starts: Use Roles, Not Individuals&lt;/h3&gt;

&lt;p&gt;The most chaotic IAM setups I’ve ever had to untangle are the ones where permissions were attached directly to individual users. As people change roles or leave, it becomes a mess of forgotten access and overly permissive accounts—a phenomenon I call “privilege creep.”&lt;/p&gt;

&lt;p&gt;I always guide teams to think in terms of roles, not people. Create IAM Groups like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Developers&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QualityEngineers&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ReadOnlyAdmins&lt;/code&gt;. Attach well-defined policies to these groups. When a new engineer joins, you add them to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Developers&lt;/code&gt; group. It’s clean, auditable, and scales beautifully.&lt;/p&gt;

&lt;h3 id=&quot;5-the-cardinal-sin-hardcoded-credentials&quot;&gt;5. The Cardinal Sin: Hardcoded Credentials&lt;/h3&gt;

&lt;p&gt;If there’s one mistake that truly keeps me up at night, it’s seeing access keys hardcoded in an application’s source code or config files. It’s the digital equivalent of taping your house key to the front door.&lt;/p&gt;

&lt;p&gt;The solution is &lt;strong&gt;IAM Roles for Services&lt;/strong&gt;. Instead of giving your EC2 instance or Lambda function a static access key, you assign it a role. This role grants the application temporary, automatically rotated credentials to access other AWS services (like an S3 bucket). Your code has the access it needs, and there are no static keys to be stolen. Consider this a non-negotiable standard for any production workload.&lt;/p&gt;

&lt;h3 id=&quot;6-the-graduation-moment-moving-to-a-multi-account-strategy&quot;&gt;6. The “Graduation” Moment: Moving to a Multi-Account Strategy&lt;/h3&gt;

&lt;p&gt;In my mind, a startup truly “graduates” to a professional cloud setup the day they move from one chaotic AWS account to a multi-account organization. I’ve seen the horror stories—a script run in a dev environment accidentally taking down a prod server because there was no separation.&lt;/p&gt;

&lt;p&gt;A multi-account strategy makes that class of error impossible. By creating separate accounts for Dev, Staging, and Production under AWS Organizations, you create powerful security boundaries. You manage your users centrally and use IAM Roles to let them switch into different environments. This isolates your workloads and is the single most effective way to limit the blast radius of any incident.&lt;/p&gt;

&lt;h3 id=&quot;my-final-take&quot;&gt;My Final Take&lt;/h3&gt;

&lt;p&gt;Building a secure foundation on AWS isn’t about complex, esoteric tools. It’s about discipline and consistently applying these core principles. The small amount of time you invest in setting up your IAM correctly today will pay massive dividends in security, scalability, and peace of mind down the road.&lt;/p&gt;</content><author><name>Rifhan Akram</name></author><category term="tech" /><summary type="html">My AWS IAM Playbook: 7 Lessons from the Trenches</summary></entry><entry><title type="html">Lessons From ER To The Software World</title><link href="https://blog.rifhanakram.com/posts/lessons-from-er-to-software-world/" rel="alternate" type="text/html" title="Lessons From ER To The Software World" /><published>2025-04-14T00:00:00+05:30</published><updated>2025-04-14T00:00:00+05:30</updated><id>https://blog.rifhanakram.com/posts/lessons-from-er-to-software-world</id><content type="html" xml:base="https://blog.rifhanakram.com/posts/lessons-from-er-to-software-world/">&lt;p&gt;What can software engineers learn from emergency rooms? In this article, I explore how the principles of ER operations—from triaging and handling surges to conducting blameless postmortems—can be applied to software incident response. Drawing parallels between medical emergencies and software incidents, I discuss the importance of proper logging, structured incident response playbooks, and continuous improvement through postmortems. The article provides practical insights for teams of any size to build more reliable software systems using lessons from emergency medicine.&lt;/p&gt;

&lt;p&gt;Read the full article on &lt;a href=&quot;https://engineering.corzent.com/when-things-go-wrong-lessons-from-er-emergency-room-to-the-software-world-9a28881c3b02&quot;&gt;Corzent’s Engineering Blog&lt;/a&gt;.&lt;/p&gt;</content><author><name>Rifhan Akram</name></author><category term="tech" /><summary type="html">What can software engineers learn from emergency rooms? In this article, I explore how the principles of ER operations—from triaging and handling surges to conducting blameless postmortems—can be applied to software incident response. Drawing parallels between medical emergencies and software incidents, I discuss the importance of proper logging, structured incident response playbooks, and continuous improvement through postmortems. The article provides practical insights for teams of any size to build more reliable software systems using lessons from emergency medicine.</summary></entry><entry><title type="html">UUIDs vs ULIDs For Better Write Performance At Scale</title><link href="https://blog.rifhanakram.com/posts/ulids-vs-uuids-for-better-write-perf/" rel="alternate" type="text/html" title="UUIDs vs ULIDs For Better Write Performance At Scale" /><published>2023-01-14T00:00:00+05:30</published><updated>2023-01-14T00:00:00+05:30</updated><id>https://blog.rifhanakram.com/posts/ulids-vs-uuids-for-better-write-perf</id><content type="html" xml:base="https://blog.rifhanakram.com/posts/ulids-vs-uuids-for-better-write-perf/">&lt;h1 id=&quot;uuids-vs-ulids-for-better-write-performance-at-scale&quot;&gt;UUIDs vs ULIDs For Better Write Performance At Scale&lt;/h1&gt;

&lt;p&gt;In this article, we will be discussing the usage of keys as unique identifiers in a distributed relational database setup and their impact on write performance.&lt;/p&gt;

&lt;p&gt;The use of auto-incremented, non-repeating, sorted integers has been a common practice for defining primary keys in relational databases. This type of key has been successful because it satisfies the two most crucial indexing requirements.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They are unique, so they can be uniquely identified.&lt;/li&gt;
  &lt;li&gt;They are ordered and sortable, so they can be efficiently indexed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When the database is a monolithic entity, this kind of key is ideal. However, as soon as we decide to scale and move to a distributed setup this is no longer unique, this is because now multiple entities start generating auto-incremented numbers.&lt;/p&gt;

&lt;h2 id=&quot;impact-on-clustered-indexes-during-writes&quot;&gt;Impact on Clustered Indexes During Writes&lt;/h2&gt;

&lt;p&gt;Before we move on to discuss alternatives to auto-incremented integers as unique identifiers, let’s try to understand how indexes are impacted when relational databases write new data.&lt;/p&gt;

&lt;p&gt;Clustered indexes are typically represented in a b-tree data structure. They have several benefits that make them well-suited for usage in clustered indexes.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Efficient at searching for data: B-trees are designed to be efficient at searching for data, even in large datasets. They use a hierarchical structure that allows for fast traversal of the data, reducing the number of disk I/O operations required to find a specific value.&lt;/li&gt;
  &lt;li&gt;B-trees are balanced: B-trees are self-balancing data structures, which means they maintain a balanced tree structure even when data is inserted or deleted. This helps to ensure that the height of the tree remains relatively constant, which improves performance and reduces the risk of data becoming inaccessible.&lt;/li&gt;
  &lt;li&gt;B-trees support range queries: B-trees can support range queries which means they can be used to efficiently find all the values that fall within a specified range. This is useful in scenarios where you need to find all the records that fall within a certain range of values for a particular column.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In most cases in relational databases, it is allowed to have only 1 clustered index per table, this is because the physical organization of the data is controlled by the clustered index(by the column(s) the index is created, typically the primary key column).&lt;/p&gt;

&lt;p&gt;In MSSQL when writing data to a table the DBMS includes logical and physical writes. A logical write is when data is modified in a page in the buffer cache and marked as dirty. A physical write is when a dirty page is written from the buffer cache to the disk. Since logical writes are not immediately written to disk there can be more than one logical write to a page in the buffer cache as long as the record is intended to be written on that page(determined by the clustered index).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/sql-writes.png&quot; alt=&quot;sql-writes.png&quot; /&gt;
&lt;em&gt;Reference - &lt;a href=&quot;https://learn.microsoft.com/en-us/sql/relational-databases/writing-pages?view=sql-server-ver16&quot;&gt;Writing Pages - SQL Server | Microsoft Learn&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The DBMS must first determine the correct page to store the new row of data based on the clustered index. To do this, it typically performs a binary search on the index to identify the correct page, once the page is identified it needs to read the page from the disk into the buffer cache, this is only required if the page is not present in the buffer cache.&lt;/p&gt;

&lt;p&gt;If the clustered index key adheres to the indexing requirements discussed above then subsequent writes will not require reading pages from the physical disk as in most cases they will be available in the buffer cache due to a previous write in near time reading it into the buffer cache.&lt;/p&gt;

&lt;p&gt;As soon as you start using a column that is not ordered(during generation) for a clustered index; It would mean that the data stored in the table requires additional work to be organized in a logical order, which would result in poor write performance.&lt;/p&gt;

&lt;h2 id=&quot;alternative-unique-identifiers-in-a-distributed-environment&quot;&gt;Alternative Unique Identifiers In a Distributed Environment&lt;/h2&gt;

&lt;h3 id=&quot;universally-unique-identifier-uuid&quot;&gt;Universally Unique Identifier (UUID)&lt;/h3&gt;

&lt;p&gt;UUIDs are 128-bit long strings that can guarantee
uniqueness across space and time. They are widely used to uniquely identify resources. There are several versions of UUIDs, each with a slightly different format. The most generally used UUID version is 4.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/uuid.png&quot; alt=&quot;uuid.png&quot; /&gt;
&lt;em&gt;128 bit UUIDV4 string&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A more detailed explanation of UUID can be found in &lt;a href=&quot;https://www.ietf.org/rfc/rfc4122.txt&quot;&gt;RFC 4122&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;UUID in a distributed setup allows uniqueness but they are not so good for write performance and here is why;&lt;/p&gt;

&lt;p&gt;UUIDs are randomized strings(in most versions) that have no particular order in a generation. When you use them for a clustered index column they need to be ordered and stored. Since they are not ordered(naturally during generation) it requires more I/O to store them on the correct page, this could result in bad write performance in large tables and it could lead to issues like &lt;a href=&quot;https://learn.microsoft.com/en-us/sql/relational-databases/indexes/reorganize-and-rebuild-indexes?view=sql-server-ver16&quot;&gt;index fragmentation&lt;/a&gt;  as the data pages may not be in contiguous order.&lt;/p&gt;

&lt;h3 id=&quot;universally-unique-lexicographically-sortable-identifier-ulid&quot;&gt;Universally Unique Lexicographically Sortable Identifier (ULID)&lt;/h3&gt;

&lt;p&gt;As the title suggests ULIDs are universally unique yet lexicographically sortable identifiers, this is mainly why it is better in write performance at scale compared to UUIDs. ULIDs are a relatively new form of identifiers and still lack widespread native support.&lt;/p&gt;

&lt;p&gt;ULID is generated in 2 components,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Timestamp - 48-bit, integer with the UNIX-time in milliseconds&lt;/li&gt;
  &lt;li&gt;Randomness - 80-bit random string&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In total, they are a 128-bit long unique yet lexicographically sortable string. ULID is encoded using a combination of binary and base32 characters which results in a more compact 26-character format compared to the 36 characters generated by UUID v4.&lt;/p&gt;

&lt;p&gt;More details about ULID can be found in its &lt;a href=&quot;[ulid/spec: The canonical spec for ulid (github.com)](https://github.com/ulid/spec)&quot;&gt;specification document&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In general, we can conclude that ULIDs tend to have the upper hand in write performance( in most cases ) compared to UUIDs, specifically due to the lexicographically sortable nature of ULID.&lt;/p&gt;

&lt;p&gt;Thank you for reading this article. I hope to see you at the next one.&lt;/p&gt;</content><author><name>Rifhan Akram</name></author><category term="tech" /><summary type="html">UUIDs vs ULIDs For Better Write Performance At Scale</summary></entry><entry><title type="html">Automated CI/CD for Monorepo’s</title><link href="https://blog.rifhanakram.com/posts/jenkins-pipeline-monorepo/" rel="alternate" type="text/html" title="Automated CI/CD for Monorepo’s" /><published>2020-10-27T00:00:00+05:30</published><updated>2020-10-27T00:00:00+05:30</updated><id>https://blog.rifhanakram.com/posts/jenkins-pipeline-monorepo</id><content type="html" xml:base="https://blog.rifhanakram.com/posts/jenkins-pipeline-monorepo/">&lt;p&gt;Monorepo is an approach in managing source code under a product, team or company within a single repo as oppose to multi-repo where a single product/application is contained within its own git repository.&lt;/p&gt;

&lt;p&gt;The two approaches has its own pro’s and con’s. In this article i will not be discussing on what is the best approach as it entirely matters on the context we work, rather i will walk the through the challenges of Automated CI/CD with &lt;strong&gt;mono-repo&lt;/strong&gt; and how we can over come them with structure and some magic with &lt;a href=&quot;https://www.jenkins.io/doc/book/pipeline/&quot;&gt;Jenkins Pipeline&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;p&gt;Due to the nature of multiple applications residing in a single repo it is not trivial to setup a CI/CD job that detects changes and executes only the &lt;strong&gt;impacted applications related CI/CD job&lt;/strong&gt;. Some of the steps that contain in a typical pipeline are,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Building the code&lt;/li&gt;
  &lt;li&gt;Running automated tests&lt;/li&gt;
  &lt;li&gt;Creating and versioning a deployable artifact&lt;/li&gt;
  &lt;li&gt;Deploying that artifact&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lets look at how we can tackle this problem with Jenkins.&lt;/p&gt;

&lt;h2 id=&quot;some-jenkins-magic&quot;&gt;Some Jenkins Magic&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;sample structure in a mono-repo setup with jenkins pipeline&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── src                                 
    ├── react-app
        ├── Jenkinsfile     
    ├── node-app
        ├── Jenkinsfile                                                               
└── Jenkinsfile                 # main jenkinsfile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we consider a structure like above, with jenkins we could setup the main Jenkinsfile in the root of the repository as a &lt;a href=&quot;https://www.jenkins.io/doc/tutorials/build-a-multibranch-pipeline-project/&quot;&gt;multi-branch jenkins job&lt;/a&gt;. This job can be connected with your remote repository such that it triggers via a webhook when a PR is merged to your mainline branch. You can get creative with the triggers depending on the workflow your team follows.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;A stage inside the main Jenkinsfile is shown below&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;stage(&apos;Execute Jobs&apos;) {
    failFast false
    parallel {
        stage(&apos;react-app ci&apos;) {
            when {
                changeset &quot;src/react-app/**&quot;
            }
            steps {
                build job: &quot;react-app-job&quot;, parameters: [string(name: &apos;branch_name&apos;, value: env.BRANCH_NAME)]
            }
        }
        stage(&apos;node-app ci&apos;) {
            when {
                changeset &quot;src/node-app/**&quot;
            }
            steps {
                build job: &quot;node-app-job&quot;, parameters: [string(name: &apos;branch_name&apos;, value: env.BRANCH_NAME)]
            }
        }
    }
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In the above example the &lt;strong&gt;changeset&lt;/strong&gt; condition walks through the git changelog for that particular change and checks if there is any change within the given path. If a change is detected then the &lt;strong&gt;build job&lt;/strong&gt; step executes the related jenkins job.&lt;/p&gt;

&lt;p&gt;Note - &lt;em&gt;&lt;strong&gt;failFast&lt;/strong&gt; is set to false to avoid failure of the entire pipeline if one job fails.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once we have this setup we can go about and setup separate pipeline jobs for each application in the repository.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Below is an example on how the main multi-branch job will look in Jenkins&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/jenkins-multi-main.png&quot; alt=&quot;jenkins-multi-main.png&quot; /&gt;
In the above image the multi-branch job has executed when a change has been pushed to the master branch. When we navigate into the master branch execution we can see the pipeline as shown below,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/img/jenkins-multi-stage-view.png&quot; alt=&quot;jenkins-multi-stage-view.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the stage view we can see that build #9 contains changes only for 1 application and build #10 contains changes for both.&lt;/p&gt;

&lt;p&gt;This is an awesome and quick setup with Jenkins for a monorepo. I’m a big fan of Jenkins due to its flexibility and extensibility specially with declarative &lt;a href=&quot;https://www.jenkins.io/doc/book/pipeline/&quot;&gt;Jenkins Pipeline&lt;/a&gt;.&lt;/p&gt;</content><author><name>Rifhan Akram</name></author><category term="tech" /><summary type="html">Monorepo is an approach in managing source code under a product, team or company within a single repo as oppose to multi-repo where a single product/application is contained within its own git repository.</summary></entry></feed>